{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e110fd4a",
   "metadata": {},
   "source": [
    "# Instacart Customer Segmentation & Reporting – Reproducible Pipeline\n",
    "\n",
    "## README – How to run\n",
    "- Place your **merged dataset from Exercise 4.9** into the `data_in/` folder at the project root. Supported formats: `.parquet` (preferred) or `.csv`.\n",
    "- Accepted file examples: `instacart_merged.parquet`, `instacart_merged.csv` (any filename is fine; the script will auto-detect the first `.parquet` or `.csv`).\n",
    "- Run the notebook top-to-bottom (**Kernel → Restart & Run All**). No interactivity or manual edits are required.\n",
    "- Expected outputs are written to subfolders:\n",
    "  - `data_out/` – exported samples and final datasets (`.parquet` + `.csv`)\n",
    "  - `figures/` – all charts as PNG (DPI=200)\n",
    "  - `reports/` – final Excel report with 7 tabs\n",
    "  - `logs/` – rotating execution log `run.log`\n",
    "- Estimated runtime and memory depend on your dataset size. The original Instacart data may take several minutes.\n",
    "\n",
    "## Contents\n",
    "1. [Environment & Imports](#Environment-&-Imports)\n",
    "2. [CONFIG](#CONFIG)\n",
    "3. [Helper Functions](#Helper-Functions)\n",
    "4. [Load & Validate Data](#Load-&-Validate-Data)\n",
    "5. [PII Handling (Hash+Salt)](#PII-Handling-(Hash+Salt))\n",
    "6. [US Regional Segmentation](#US-Regional-Segmentation)\n",
    "7. [Exclude Low-Activity Customers](#Exclude-Low-Activity-Customers)\n",
    "8. [Rule-based Customer Profiles](#Rule-based-Customer-Profiles)\n",
    "9. [Analytics & Statistical Tests](#Analytics-&-Statistical-Tests)\n",
    "10. [Visualizations](#Visualizations)\n",
    "11. [Export Final Datasets](#Export-Final-Datasets)\n",
    "12. [Excel Report (7 Tabs)](#Excel-Report-(7-Tabs))\n",
    "13. [Run Summary](#Run-Summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf693afa",
   "metadata": {},
   "source": [
    "# Environment & Imports\n",
    "_Set deterministic seed, configure logging, and print library versions._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdbd6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from dotenv import load_dotenv, set_key\n",
    "import hashlib\n",
    "from textwrap import fill\n",
    "from logging.handlers import RotatingFileHandler\n",
    "import logging\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.drawing.image import Image as XLImage\n",
    "from openpyxl.utils import get_column_letter\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "\n",
    "print('Python', sys.version)\n",
    "print('pandas', pd.__version__)\n",
    "print('numpy', np.__version__)\n",
    "print('matplotlib', plt.matplotlib.__version__)\n",
    "print('seaborn', sns.__version__)\n",
    "print('scipy present')\n",
    "import openpyxl\n",
    "print('openpyxl', openpyxl.__version__)\n",
    "print('pyarrow', pa.__version__)\n",
    "import dotenv\n",
    "print('python-dotenv', getattr(dotenv, '__version__', 'present'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b539eea5",
   "metadata": {},
   "source": [
    "# CONFIG\n",
    "_Project-wide configuration: folders, thresholds, department groups, schema, and plotting settings._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ddd1113",
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG = {\n",
    "    'paths': {\n",
    "        'data_in': 'data_in',\n",
    "        'data_out': 'data_out',\n",
    "        'figures': 'figures',\n",
    "        'reports': 'reports',\n",
    "        'logs': 'logs',\n",
    "        'env_file': '.env',\n",
    "    },\n",
    "    'files': {\n",
    "        'active_customers_sample_base': 'active_customers_sample',\n",
    "        'final_dataset_base': 'final_instacart_dataset',\n",
    "        'excel_report': 'Instacart_Final_Report.xlsx',\n",
    "    },\n",
    "    'required_columns': [\n",
    "        'customer_id','state','orders_count','order_id','order_number',\n",
    "        'order_hour_of_day','order_dow','age','income','dependents',\n",
    "        'department_id','price','spending_flag'\n",
    "    ],\n",
    "    'column_aliases': {\n",
    "        'State': 'state',\n",
    "        'orders_day_of_week': 'order_dow',\n",
    "        'order_hour': 'order_hour_of_day',\n",
    "        'n_dependents': 'dependents',\n",
    "        'department': 'department_id',\n",
    "        'prices': 'price',\n",
    "        'order_price': 'price'\n",
    "    },\n",
    "    'pii_patterns': ['first_name','last_name','full_name','email','phone','address','street','zip','zipcode'],\n",
    "    'region_mapping': {\n",
    "        'Northeast': ['CT','ME','MA','NH','RI','VT','NJ','NY','PA','DC'],\n",
    "        'Midwest': ['IL','IN','MI','OH','WI','IA','KS','MN','MO','NE','ND','SD'],\n",
    "        'South': ['DE','FL','GA','MD','NC','SC','VA','WV','AL','KY','MS','TN','AR','LA','OK','TX'],\n",
    "        'West': ['AZ','CO','ID','MT','NV','NM','UT','WY','AK','CA','HI','OR','WA']\n",
    "    },\n",
    "    # Optional: if departments.csv is available in data_in/, these labels can be verified/overridden\n",
    "    'dept_groups': {\n",
    "        # Examples; adjust if your data uses different IDs\n",
    "        'young_parent_depts': {3, 4, 7, 10, 12},\n",
    "        'health_depts': {4, 16}  # e.g., produce=4, dairy/eggs=16 in common Instacart mappings\n",
    "    },\n",
    "    'thresholds': {\n",
    "        'young_parent_age_min': 20,\n",
    "        'young_parent_age_max': 34,\n",
    "        'evening_hours': list(range(18, 23)),\n",
    "        'evening_share_min': 0.30,\n",
    "        'morning_hours': list(range(6, 11)),\n",
    "        'morning_share_min': 0.30,\n",
    "        'weekday_hours': list(range(9, 18)),\n",
    "        'weekday_share_min': 0.50,\n",
    "        'late_hours': list(range(20, 25)),\n",
    "        'late_share_min': 0.30,\n",
    "        'weekend_dows': {0, 6},  # Sunday=0 in Instacart, Saturday=6\n",
    "        'weekend_share_min': 0.35,\n",
    "        'young_parent_dept_share_min': 0.20,\n",
    "        'health_dept_share_min': 0.25,\n",
    "        'family_saver_income_max': 50000,\n",
    "        'affluent_income_min': 120000,\n",
    "        'senior_age_min': 65\n",
    "    },\n",
    "    'plot': {\n",
    "        'style': 'whitegrid',\n",
    "        'font_scale': 1.1,\n",
    "        'dpi': 200,\n",
    "        'figsize': (10, 6)\n",
    "    }\n",
    "}\n",
    "\n",
    "def setup_environment(cfg: dict) -> logging.Logger:\n",
    "    for key in ['data_in', 'data_out', 'figures', 'reports', 'logs']:\n",
    "        Path(cfg['paths'][key]).mkdir(parents=True, exist_ok=True)\n",
    "    # Logging\n",
    "    log_path = Path(cfg['paths']['logs']) / 'run.log'\n",
    "    logger = logging.getLogger('instacart')\n",
    "    logger.setLevel(logging.INFO)\n",
    "    # Avoid duplicate handlers if re-running\n",
    "    logger.handlers.clear()\n",
    "    fh = RotatingFileHandler(log_path, maxBytes=2_000_000, backupCount=3)\n",
    "    fmt = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "    fh.setFormatter(fmt)\n",
    "    logger.addHandler(fh)\n",
    "    ch = logging.StreamHandler(sys.stdout)\n",
    "    ch.setFormatter(fmt)\n",
    "    logger.addHandler(ch)\n",
    "    logger.info('Environment initialized.')\n",
    "    return logger\n",
    "\n",
    "LOGGER = setup_environment(CONFIG)\n",
    "\n",
    "# Global plotting theme\n",
    "sns.set_theme(style=CONFIG['plot']['style'])\n",
    "sns.set_context('notebook')\n",
    "plt.rcParams['figure.dpi'] = CONFIG['plot']['dpi']\n",
    "plt.rcParams['savefig.dpi'] = CONFIG['plot']['dpi']\n",
    "plt.rcParams['font.size'] = 11\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abf6088",
   "metadata": {},
   "source": [
    "# Helper Functions\n",
    "_Modular utilities: file discovery, IO, schema checks, PII hashing, region mapping, profiling, analytics, plotting, and reporting._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7f4e08c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_first_file(folder: Path, extensions: tuple) -> Path:\n",
    "    candidates = []\n",
    "    for ext in extensions:\n",
    "        candidates.extend(sorted(folder.glob(f'*{ext}')))\n",
    "    return candidates[0] if candidates else None\n",
    "\n",
    "def load_dataset_auto(cfg: dict) -> tuple[pd.DataFrame, str]:\n",
    "    data_in = Path(cfg['paths']['data_in'])\n",
    "    p = find_first_file(data_in, ('.parquet', '.pq'))\n",
    "    fmt = None\n",
    "    if p is not None:\n",
    "        fmt = 'parquet'\n",
    "    else:\n",
    "        p = find_first_file(data_in, ('.csv',))\n",
    "        if p is not None:\n",
    "            fmt = 'csv'\n",
    "    if p is None:\n",
    "        raise FileNotFoundError(\n",
    "            'No input file found in data_in/. Expected a merged dataset (.parquet preferred or .csv).')\n",
    "    LOGGER.info(f'Loading dataset: {p.name} (format={fmt})')\n",
    "    if fmt == 'parquet':\n",
    "        df = pd.read_parquet(p, engine='pyarrow')\n",
    "    else:\n",
    "        df = pd.read_csv(p)\n",
    "    return df, p.name\n",
    "\n",
    "def normalize_columns(df: pd.DataFrame, cfg: dict) -> tuple[pd.DataFrame, dict]:\n",
    "    original_cols = df.columns.tolist()\n",
    "    col_map = {}\n",
    "    # Lowercase normalization first\n",
    "    lower_map = {c: c.lower() for c in df.columns}\n",
    "    df = df.rename(columns=lower_map)\n",
    "    # Apply explicit aliases\n",
    "    for k, v in cfg['column_aliases'].items():\n",
    "        if k.lower() in df.columns and v not in df.columns:\n",
    "            df = df.rename(columns={k.lower(): v})\n",
    "            col_map[k] = v\n",
    "    # Ensure required presence or prepare to compute\n",
    "    # Dependents: if missing, add and mark\n",
    "    added_dependents = False\n",
    "    if 'dependents' not in df.columns:\n",
    "        df['dependents'] = 0\n",
    "        added_dependents = True\n",
    "    # spending_flag may be computed later if missing\n",
    "    LOGGER.info(f'Columns before normalization: {original_cols}')\n",
    "    LOGGER.info(f'Columns after normalization: {df.columns.tolist()}')\n",
    "    return df, {'renamed': col_map, 'added_dependents': added_dependents}\n",
    "\n",
    "def validate_required_columns(df: pd.DataFrame, cfg: dict):\n",
    "    required = set(cfg['required_columns'])\n",
    "    present = set(df.columns)\n",
    "    missing = required - present\n",
    "    if missing:\n",
    "        raise ValueError(\n",
    "            'Schema validation failed. Missing required columns: ' + ', '.join(sorted(missing)) +\n",
    "            '. Use aliases in CONFIG if your file uses alternative names.')\n",
    "    # Basic types coercion where safe\n",
    "    for col in ['order_hour_of_day', 'order_dow', 'order_number', 'orders_count', 'department_id']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    for col in ['age', 'income', 'price']:\n",
    "        if col in df.columns:\n",
    "            df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "    if df[['customer_id','order_id']].isna().any().any():\n",
    "        raise ValueError('Nulls found in key columns customer_id/order_id. Please fix source data.')\n",
    "\n",
    "def compute_spending_flag_if_missing(df: pd.DataFrame, cfg: dict) -> tuple[pd.DataFrame, bool]:\n",
    "    created = False\n",
    "    if 'spending_flag' not in df.columns:\n",
    "        LOGGER.info('spending_flag not found. Creating by Q3 total spend per customer (1=high spender).')\n",
    "        cust_spend = df.groupby('customer_id')['price'].sum().rename('total_spend_per_customer')\n",
    "        q3 = cust_spend.quantile(0.75)\n",
    "        high = (cust_spend >= q3).astype(int).rename('spending_flag')\n",
    "        df = df.merge(high, on='customer_id', how='left')\n",
    "        created = True\n",
    "    return df, created\n",
    "\n",
    "def get_or_create_salt(env_path: Path) -> str:\n",
    "    load_dotenv(dotenv_path=env_path)\n",
    "    salt = os.getenv('ANON_SALT')\n",
    "    if not salt:\n",
    "        # Generate cryptographically strong salt; store for reproducibility across runs on the same project\n",
    "        import secrets\n",
    "        salt = secrets.token_hex(16)\n",
    "        set_key(str(env_path), 'ANON_SALT', salt)\n",
    "        LOGGER.info(f'Generated new salt and stored in {env_path}')\n",
    "    else:\n",
    "        LOGGER.info(f'Using existing salt from {env_path}')\n",
    "    return salt\n",
    "\n",
    "def hash_pii_columns(df: pd.DataFrame, cfg: dict) -> tuple[pd.DataFrame, list[str]]:\n",
    "    patterns = set([p.lower() for p in cfg['pii_patterns']])\n",
    "    pii_cols = [c for c in df.columns if any(p == c.lower() for p in patterns)]\n",
    "    if not pii_cols:\n",
    "        LOGGER.info('No PII columns matched the patterns.')\n",
    "        return df, []\n",
    "    salt = get_or_create_salt(Path(cfg['paths']['env_file']))\n",
    "    def _hash_val(x):\n",
    "        if pd.isna(x):\n",
    "            return x\n",
    "        h = hashlib.sha256((salt + str(x)).encode('utf-8')).hexdigest()\n",
    "        return 'hash_' + h[:12]\n",
    "    for col in pii_cols:\n",
    "        df[col] = df[col].map(_hash_val)\n",
    "    LOGGER.info(f'Applied irreversible SHA-256 hashing with salt to PII columns: {pii_cols}')\n",
    "    return df, pii_cols\n",
    "\n",
    "US_STATE_ABBR = {\n",
    "    'ALABAMA':'AL','ALASKA':'AK','ARIZONA':'AZ','ARKANSAS':'AR','CALIFORNIA':'CA','COLORADO':'CO','CONNECTICUT':'CT',\n",
    "    'DELAWARE':'DE','DISTRICT OF COLUMBIA':'DC','FLORIDA':'FL','GEORGIA':'GA','HAWAII':'HI','IDAHO':'ID','ILLINOIS':'IL',\n",
    "    'INDIANA':'IN','IOWA':'IA','KANSAS':'KS','KENTUCKY':'KY','LOUISIANA':'LA','MAINE':'ME','MARYLAND':'MD','MASSACHUSETTS':'MA',\n",
    "    'MICHIGAN':'MI','MINNESOTA':'MN','MISSISSIPPI':'MS','MISSOURI':'MO','MONTANA':'MT','NEBRASKA':'NE','NEVADA':'NV',\n",
    "    'NEW HAMPSHIRE':'NH','NEW JERSEY':'NJ','NEW MEXICO':'NM','NEW YORK':'NY','NORTH CAROLINA':'NC','NORTH DAKOTA':'ND',\n",
    "    'OHIO':'OH','OKLAHOMA':'OK','OREGON':'OR','PENNSYLVANIA':'PA','RHODE ISLAND':'RI','SOUTH CAROLINA':'SC','SOUTH DAKOTA':'SD',\n",
    "    'TENNESSEE':'TN','TEXAS':'TX','UTAH':'UT','VERMONT':'VT','VIRGINIA':'VA','WASHINGTON':'WA','WEST VIRGINIA':'WV','WISCONSIN':'WI','WYOMING':'WY'\n",
    "}\n",
    "\n",
    "def map_state_and_region(df: pd.DataFrame, cfg: dict) -> tuple[pd.DataFrame, dict]:\n",
    "    df = df.copy()\n",
    "    # Normalize 'state' to two-letter abbreviations\n",
    "    def to_abbr(s):\n",
    "        if pd.isna(s):\n",
    "            return None\n",
    "        s = str(s).strip().upper()\n",
    "        if len(s) == 2 and s.isalpha():\n",
    "            return s\n",
    "        return US_STATE_ABBR.get(s, None)\n",
    "    df['state'] = df['state'].map(to_abbr)\n",
    "    # Map to region\n",
    "    rev = {}\n",
    "    for region, states in cfg['region_mapping'].items():\n",
    "        for st in states:\n",
    "            rev[st] = region\n",
    "    df['region'] = df['state'].map(rev)\n",
    "    before = len(df)\n",
    "    excluded = df[df['region'].isna()].copy()\n",
    "    df = df[~df['region'].isna()].copy()\n",
    "    after = len(df)\n",
    "    exclusion_info = {\n",
    "        'excluded_count': int(before - after),\n",
    "        'excluded_reason': 'Unknown or non-US state',\n",
    "        'examples': excluded['state'].dropna().unique().tolist()[:10]\n",
    "    }\n",
    "    if exclusion_info['excluded_count'] > 0:\n",
    "        LOGGER.warning(f\"Excluded {exclusion_info['excluded_count']} rows due to unknown/non-US states: {exclusion_info['examples']}\")\n",
    "    return df, exclusion_info\n",
    "\n",
    "def add_low_activity_flag(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    df['low_activity_flag'] = (df['orders_count'] < 5).astype(int)\n",
    "    return df\n",
    "\n",
    "def build_customer_features(df: pd.DataFrame, cfg: dict) -> pd.DataFrame:\n",
    "    # Per-order totals\n",
    "    order_totals = df.groupby('order_id', as_index=False)['price'].sum().rename(columns={'price': 'order_total'})\n",
    "    cust_orders = df[['order_id', 'customer_id', 'order_hour_of_day', 'order_dow']].drop_duplicates()\n",
    "    cust_orders = cust_orders.merge(order_totals, on='order_id', how='left')\n",
    "    # Aggregations per customer\n",
    "    t = cfg['thresholds']\n",
    "    def share_in(values, allowed):\n",
    "        if len(values) == 0:\n",
    "            return 0.0\n",
    "        return float(np.isin(values, list(allowed)).mean())\n",
    "    cust_hour = cust_orders.groupby('customer_id').agg(\n",
    "        orders=('order_id','nunique'),\n",
    "        median_order_total=('order_total','median'),\n",
    "        share_evening=('order_hour_of_day', lambda x: share_in(x.values, t['evening_hours'])),\n",
    "        share_morning=('order_hour_of_day', lambda x: share_in(x.values, t['morning_hours'])),\n",
    "        share_weekday=('order_hour_of_day', lambda x: share_in(x.values, t['weekday_hours'])),\n",
    "        share_late=('order_hour_of_day', lambda x: share_in(x.values, t['late_hours'])),\n",
    "        share_weekend=('order_dow', lambda x: share_in(x.values, t['weekend_dows']))\n",
    "    ).reset_index()\n",
    "    # Department shares\n",
    "    dept_counts = df.groupby(['customer_id','department_id']).size().rename('n').reset_index()\n",
    "    total_counts = dept_counts.groupby('customer_id')['n'].sum().rename('total_n').reset_index()\n",
    "    dept_counts = dept_counts.merge(total_counts, on='customer_id', how='left')\n",
    "    dept_counts['share'] = dept_counts['n'] / dept_counts['total_n']\n",
    "    # Shares for configured sets\n",
    "    yp_set = set(cfg['dept_groups']['young_parent_depts'])\n",
    "    hl_set = set(cfg['dept_groups']['health_depts'])\n",
    "    yp_share = dept_counts[dept_counts['department_id'].isin(yp_set)].groupby('customer_id')['share'].sum().rename('share_young_parent_depts')\n",
    "    hl_share = dept_counts[dept_counts['department_id'].isin(hl_set)].groupby('customer_id')['share'].sum().rename('share_health_depts')\n",
    "    shares = pd.concat([yp_share, hl_share], axis=1).fillna(0.0).reset_index()\n",
    "    # Total spend per customer\n",
    "    spend = df.groupby('customer_id')['price'].sum().rename('total_spend_per_customer').reset_index()\n",
    "    # Global median order total\n",
    "    global_median_order_total = cust_orders['order_total'].median()\n",
    "    # Combine\n",
    "    feat = cust_hour.merge(shares, on='customer_id', how='left')\\\n",
    "                   .merge(spend, on='customer_id', how='left')\n",
    "    feat['low_price_baskets'] = (feat['median_order_total'] < global_median_order_total).astype(int)\n",
    "    feat['global_median_order_total'] = global_median_order_total\n",
    "    return feat\n",
    "\n",
    "def assign_profiles(df: pd.DataFrame, feats: pd.DataFrame, cfg: dict) -> pd.DataFrame:\n",
    "    t = cfg['thresholds']\n",
    "    cust = df[['customer_id','age','income','dependents','spending_flag']].drop_duplicates('customer_id')\n",
    "    cust = cust.merge(feats, on='customer_id', how='left')\n",
    "    profile = []\n",
    "    for _, r in cust.iterrows():\n",
    "        age = r['age']\n",
    "        inc = r['income']\n",
    "        deps = r['dependents']\n",
    "        spf = r['spending_flag']\n",
    "        se = r['share_evening']; sm = r['share_morning']; sw = r['share_weekday']\n",
    "        sl = r['share_late']; swk = r['share_weekend']\n",
    "        yp_share = r.get('share_young_parent_depts', 0.0)\n",
    "        hl_share = r.get('share_health_depts', 0.0)\n",
    "        low_price = r['low_price_baskets']\n",
    "        med_ord = r['median_order_total']; glob_med = r['global_median_order_total']\n",
    "        # Rule 1\n",
    "        if (t['young_parent_age_min'] <= age <= t['young_parent_age_max'] and deps >= 1 and\n",
    "            yp_share >= t['young_parent_dept_share_min'] and se >= t['evening_share_min']):\n",
    "            profile.append('Young Parent')\n",
    "            continue\n",
    "        # Rule 2\n",
    "        if (deps >= 2 and inc < t['family_saver_income_max'] and (low_price == 1 or spf == 0)):\n",
    "            profile.append('Family Saver')\n",
    "            continue\n",
    "        # Rule 3\n",
    "        if (inc >= t['affluent_income_min'] and sw >= t['weekday_share_min'] and med_ord > glob_med):\n",
    "            profile.append('Affluent Professional')\n",
    "            continue\n",
    "        # Rule 4\n",
    "        if (hl_share >= t['health_dept_share_min'] and sm >= t['morning_share_min']):\n",
    "            profile.append('Health Conscious')\n",
    "            continue\n",
    "        # Rule 5\n",
    "        if (deps == 0 and 25 <= age <= 44 and (sl >= t['late_share_min'] or swk >= t['weekend_share_min'])):\n",
    "            profile.append('Single Adult')\n",
    "            continue\n",
    "        # Rule 6\n",
    "        if (age >= t['senior_age_min']):\n",
    "            profile.append('Senior')\n",
    "            continue\n",
    "        profile.append('Other')\n",
    "    cust['profile'] = profile\n",
    "    assert cust['profile'].isna().sum() == 0, 'Profile assignment produced NaN values.'\n",
    "    # Return per-customer profiles; merge back to row-level\n",
    "    out = df.merge(cust[['customer_id','profile']], on='customer_id', how='left')\n",
    "    return out, cust\n",
    "\n",
    "def region_spend_stats(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    cust_spend = df.groupby(['customer_id','region'])['price'].sum().rename('total_spend_per_customer').reset_index()\n",
    "    stats_tbl = cust_spend.groupby('region')['total_spend_per_customer'].agg(\n",
    "        count='count', mean='mean', median='median', std='std', q1=lambda x: x.quantile(0.25), q3=lambda x: x.quantile(0.75)\n",
    "    ).reset_index()\n",
    "    return stats_tbl, cust_spend\n",
    "\n",
    "def stat_tests_by_region(cust_spend: pd.DataFrame) -> dict:\n",
    "    groups = [g['total_spend_per_customer'].values for _, g in cust_spend.groupby('region')]\n",
    "    # ANOVA\n",
    "    anova = stats.f_oneway(*groups)\n",
    "    # Kruskal-Wallis\n",
    "    kruskal = stats.kruskal(*groups)\n",
    "    return {\n",
    "        'anova_f': float(anova.statistic), 'anova_p': float(anova.pvalue),\n",
    "        'kruskal_h': float(kruskal.statistic), 'kruskal_p': float(kruskal.pvalue)\n",
    "    }\n",
    "\n",
    "def plot_boxplot_spend_by_region(cust_spend: pd.DataFrame, cfg: dict, save_path: Path):\n",
    "    plt.figure(figsize=cfg['plot']['figsize'])\n",
    "    ax = sns.boxplot(data=cust_spend, x='region', y='total_spend_per_customer')\n",
    "    ax.set_title('Total Spend per Customer by Region')\n",
    "    ax.set_xlabel('Region')\n",
    "    ax.set_ylabel('Total Spend per Customer')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_violin_spend_by_region(cust_spend: pd.DataFrame, cfg: dict, save_path: Path):\n",
    "    plt.figure(figsize=cfg['plot']['figsize'])\n",
    "    ax = sns.violinplot(data=cust_spend, x='region', y='total_spend_per_customer', cut=0)\n",
    "    ax.set_title('Distribution of Total Spend per Customer by Region (Violin)')\n",
    "    ax.set_xlabel('Region')\n",
    "    ax.set_ylabel('Total Spend per Customer')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def plot_profile_distribution(cust_profiles: pd.DataFrame, cfg: dict, save_path: Path):\n",
    "    dist = cust_profiles['profile'].value_counts().rename_axis('profile').reset_index(name='count')\n",
    "    dist['share'] = dist['count'] / dist['count'].sum()\n",
    "    plt.figure(figsize=cfg['plot']['figsize'])\n",
    "    ax = sns.barplot(data=dist, x='profile', y='count')\n",
    "    ax.set_title('Customer Profile Distribution')\n",
    "    ax.set_xlabel('Profile')\n",
    "    ax.set_ylabel('Count')\n",
    "    for i, row in dist.iterrows():\n",
    "        ax.text(i, row['count'], f\"{row['count']}\\n({row['share']:.1%})\", ha='center', va='bottom')\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def crosstab_profile_region(cust_profiles: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    ct = pd.crosstab(cust_profiles['profile'], cust_profiles['region'])\n",
    "    ct_row = ct.div(ct.sum(axis=1), axis=0).fillna(0.0)\n",
    "    ct_col = ct.div(ct.sum(axis=0), axis=1).fillna(0.0)\n",
    "    return ct_row, ct_col\n",
    "\n",
    "def plot_heatmap(df: pd.DataFrame, title: str, cfg: dict, save_path: Path):\n",
    "    plt.figure(figsize=(max(cfg['plot']['figsize'][0], 10), max(cfg['plot']['figsize'][1], 7)))\n",
    "    ax = sns.heatmap(df, annot=True, fmt='.1%', cmap='viridis')\n",
    "    ax.set_title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def top_departments_by_profile(df: pd.DataFrame, cust_profiles: pd.DataFrame, top_n: int = 10) -> pd.DataFrame:\n",
    "    tmp = df.merge(cust_profiles[['customer_id','profile']], on='customer_id', how='left')\n",
    "    counts = tmp.groupby(['profile','department_id']).size().rename('n').reset_index()\n",
    "    counts['share'] = counts.groupby('profile')['n'].apply(lambda x: x / x.sum())\n",
    "    counts = counts.sort_values(['profile','share'], ascending=[True, False])\n",
    "    # Keep top N per profile\n",
    "    counts['rank'] = counts.groupby('profile')['share'].rank(method='first', ascending=False)\n",
    "    return counts[counts['rank'] <= top_n].drop(columns='rank')\n",
    "\n",
    "def profile_department_heatmap(df: pd.DataFrame, cust_profiles: pd.DataFrame) -> pd.DataFrame:\n",
    "    tmp = df.merge(cust_profiles[['customer_id','profile']], on='customer_id', how='left')\n",
    "    ct = pd.crosstab(tmp['profile'], tmp['department_id'])\n",
    "    ct = ct.div(ct.sum(axis=1), axis=0).fillna(0.0)\n",
    "    return ct\n",
    "\n",
    "def export_dataframe(df: pd.DataFrame, base_path: Path):\n",
    "    df.to_parquet(base_path.with_suffix('.parquet'), engine='pyarrow', index=False)\n",
    "    df.to_csv(base_path.with_suffix('.csv'), index=False)\n",
    "\n",
    "def write_excel_report(cfg: dict,\n",
    "                       meta: dict,\n",
    "                       population_flow: pd.DataFrame,\n",
    "                       consistency_checks: pd.DataFrame,\n",
    "                       wrangling_security: pd.DataFrame,\n",
    "                       derived_columns: pd.DataFrame,\n",
    "                       figures: list[tuple[str, Path]],\n",
    "                       results_reco: list[str],\n",
    "                       out_path: Path,\n",
    "                       crosstab_sf_region: pd.DataFrame | None = None,\n",
    "                       top_depts: pd.DataFrame | None = None):\n",
    "    wb = Workbook()\n",
    "    # Tab1 Project Details & Sources\n",
    "    ws = wb.active\n",
    "    ws.title = 'Project Details & Sources'\n",
    "    rows = [\n",
    "        ['Project', 'Instacart – Customer Segmentation & Reporting'],\n",
    "        ['Dataset Version', meta.get('dataset_name','')],\n",
    "        ['Generated At', meta.get('generated_at','')],\n",
    "        ['Library Versions', meta.get('lib_versions','')],\n",
    "        ['PII Note', meta.get('pii_note','')],\n",
    "        ['Sources', 'Instacart public dataset (merged), project Exercise 4.9 output']\n",
    "    ]\n",
    "    for r in rows:\n",
    "        ws.append(r)\n",
    "\n",
    "    # Tab2 Population Flow\n",
    "    ws2 = wb.create_sheet('Population Flow')\n",
    "    for r in [population_flow.columns.tolist()] + population_flow.values.tolist():\n",
    "        ws2.append(list(r))\n",
    "\n",
    "    # Tab3 Consistency Checks\n",
    "    ws3 = wb.create_sheet('Consistency Checks')\n",
    "    for r in [consistency_checks.columns.tolist()] + consistency_checks.values.tolist():\n",
    "        ws3.append(list(r))\n",
    "    # Append Crosstab spending_flag x region if provided\n",
    "    if crosstab_sf_region is not None:\n",
    "        ws3.append([])\n",
    "        ws3.append(['Crosstab: spending_flag × region'])\n",
    "        cols = ['spending_flag'] + list(map(str, crosstab_sf_region.columns.tolist()))\n",
    "        ws3.append(cols)\n",
    "        for idx, row in crosstab_sf_region.iterrows():\n",
    "            ws3.append([str(idx)] + row.tolist())\n",
    "\n",
    "    # Tab4 Data Wrangling & Security\n",
    "    ws4 = wb.create_sheet('Data Wrangling & Security')\n",
    "    for r in [wrangling_security.columns.tolist()] + wrangling_security.values.tolist():\n",
    "        ws4.append(list(r))\n",
    "\n",
    "    # Tab5 Derived Columns\n",
    "    ws5 = wb.create_sheet('Derived Columns')\n",
    "    for r in [derived_columns.columns.tolist()] + derived_columns.values.tolist():\n",
    "        ws5.append(list(r))\n",
    "\n",
    "    # Tab6 Visualizations – insert images\n",
    "    ws6 = wb.create_sheet('Visualizations')\n",
    "    cur_row = 1\n",
    "    for caption, fig_path in figures:\n",
    "        ws6.cell(row=cur_row, column=1, value=caption)\n",
    "        cur_row += 1\n",
    "        if fig_path.exists():\n",
    "            img = XLImage(str(fig_path))\n",
    "            img.anchor = f'A{cur_row}'\n",
    "            ws6.add_image(img)\n",
    "            cur_row += int( img.height / 20 ) + 2  # crude spacing\n",
    "        else:\n",
    "            ws6.cell(row=cur_row, column=1, value=f'File not found: {fig_path.name}')\n",
    "            cur_row += 2\n",
    "\n",
    "    # Optional: Top departments by profile\n",
    "    if top_depts is not None:\n",
    "        ws8 = wb.create_sheet('Top Depts by Profile')\n",
    "        ws8.append(top_depts.columns.tolist())\n",
    "        for r in top_depts.itertuples(index=False):\n",
    "            ws8.append(list(r))\n",
    "\n",
    "    # Tab7 Results & Recommendations\n",
    "    ws7 = wb.create_sheet('Results & Recommendations')\n",
    "    ws7.append(['Recommendations'])\n",
    "    for item in results_reco:\n",
    "        ws7.append([item])\n",
    "\n",
    "    # Save\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    wb.save(out_path)\n",
    "\n",
    "def summarize_artifacts(paths: dict):\n",
    "    print('\\n=== Artifacts ===')\n",
    "    for k, v in paths.items():\n",
    "        print(f\"{k}: {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358d8e13",
   "metadata": {},
   "source": [
    "# Load & Validate Data\n",
    "_Auto-detect input file (.parquet preferred, else .csv), normalize columns, and validate schema. Compute `spending_flag` if missing._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9a66d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_META = {}\n",
    "RUN_META['start_time'] = datetime.now().isoformat(timespec='seconds')\n",
    "df_raw, dataset_name = load_dataset_auto(CONFIG)\n",
    "RUN_META['dataset_name'] = dataset_name\n",
    "\n",
    "df, norm_info = normalize_columns(df_raw, CONFIG)\n",
    "df, spending_created = compute_spending_flag_if_missing(df, CONFIG)\n",
    "validate_required_columns(df, CONFIG)\n",
    "LOGGER.info(f\"Normalization info: {norm_info}; spending_flag created: {spending_created}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3d7d737",
   "metadata": {},
   "source": [
    "# PII Handling (Hash+Salt)\n",
    "_Detect PII columns by pattern and irreversibly hash them with SHA-256 using a stored salt in `.env`. The anonymized dataframe is used downstream._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "364bbddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_anon, pii_cols = hash_pii_columns(df, CONFIG)\n",
    "RUN_META['pii_cols'] = pii_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0525249d",
   "metadata": {},
   "source": [
    "# US Regional Segmentation\n",
    "_Map `state` to USPS abbreviations and then to US Census regions; exclude unknown/non-US with logging._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbce757f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pop_flow = []\n",
    "pop_flow.append(('Start', len(df_anon)))\n",
    "\n",
    "df_reg, excl_info = map_state_and_region(df_anon, CONFIG)\n",
    "pop_flow.append(('After State Mapping', len(df_reg)))\n",
    "RUN_META['excluded_states'] = excl_info\n",
    "LOGGER.info(f\"Population flow so far: {pop_flow}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b49a3fd",
   "metadata": {},
   "source": [
    "# Exclude Low-Activity Customers\n",
    "_Create `low_activity_flag` for customers with `< 5` orders and export the active sample._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed2e04b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reg = add_low_activity_flag(df_reg)\n",
    "active = df_reg[df_reg['low_activity_flag'] == 0].copy()\n",
    "pop_flow.append(('Exclude Low Activity (>=5 orders)', len(active)))\n",
    "LOGGER.info(f\"Population flow with activity filter: {pop_flow}\")\n",
    "\n",
    "# Export active customers sample (row-level subset)\n",
    "active_base = Path(CONFIG['paths']['data_out']) / CONFIG['files']['active_customers_sample_base']\n",
    "export_dataframe(active, active_base)\n",
    "ACTIVE_SAMPLE_PARQUET = str(active_base.with_suffix('.parquet'))\n",
    "ACTIVE_SAMPLE_CSV = str(active_base.with_suffix('.csv'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523e74c3",
   "metadata": {},
   "source": [
    "# Rule-based Customer Profiles\n",
    "_Build per-customer behavioral features (hours, departments, spend), then assign profiles by prioritized rules. Validate no missing profiles._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc53ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = build_customer_features(df_reg, CONFIG)\n",
    "df_prof, cust_prof = assign_profiles(df_reg, features, CONFIG)\n",
    "assert cust_prof['profile'].isna().sum() == 0, 'Profile assignment has NaNs.'\n",
    "pop_flow.append(('Profiled Customers', cust_prof.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56bab1a8",
   "metadata": {},
   "source": [
    "# Analytics & Statistical Tests\n",
    "_Regional spend differences (summary, ANOVA, Kruskal–Wallis); profile distributions and aggregates._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10cb22a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats_tbl, cust_spend = region_spend_stats(df_prof)\n",
    "tests = stat_tests_by_region(cust_spend)\n",
    "\n",
    "# Aggregates by profile\n",
    "agg_by_profile = cust_prof.groupby('profile').agg(\n",
    "    min_orders=('orders','min'), mean_orders=('orders','mean'), max_orders=('orders','max'),\n",
    "    min_expenditure=('total_spend_per_customer','min'),\n",
    "    mean_expenditure=('total_spend_per_customer','mean'),\n",
    "    max_expenditure=('total_spend_per_customer','max')\n",
    ").reset_index()\n",
    "\n",
    "display(stats_tbl.head())\n",
    "print('ANOVA F={:.4f}, p-value={:.6f}'.format(tests['anova_f'], tests['anova_p']))\n",
    "print('Kruskal H={:.4f}, p-value={:.6f}'.format(tests['kruskal_h'], tests['kruskal_p']))\n",
    "display(agg_by_profile.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29d82bbf",
   "metadata": {},
   "source": [
    "# Visualizations\n",
    "_Save all figures in `figures/` with consistent styling (seaborn whitegrid, font size 11, PNG, DPI=200)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554cca31",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_dir = Path(CONFIG['paths']['figures'])\n",
    "box_path = fig_dir / 'boxplot_spend_by_region.png'\n",
    "violin_path = fig_dir / 'violin_spend_by_region.png'\n",
    "prof_bar_path = fig_dir / 'barplot_profile_distribution.png'\n",
    "heat_row_path = fig_dir / 'heatmap_profile_x_region_rownorm.png'\n",
    "heat_col_path = fig_dir / 'heatmap_profile_x_region_colnorm.png'\n",
    "heat_dept_path = fig_dir / 'heatmap_profile_x_department.png'\n",
    "\n",
    "plot_boxplot_spend_by_region(cust_spend, CONFIG, box_path)\n",
    "plot_violin_spend_by_region(cust_spend, CONFIG, violin_path)\n",
    "plot_profile_distribution(cust_prof, CONFIG, prof_bar_path)\n",
    "\n",
    "ct_row, ct_col = crosstab_profile_region(cust_prof.merge(df_prof[['customer_id','region']].drop_duplicates(), on='customer_id', how='left'))\n",
    "plot_heatmap(ct_row, 'Profile vs Region (Row-normalized)', CONFIG, heat_row_path)\n",
    "plot_heatmap(ct_col, 'Profile vs Region (Column-normalized)', CONFIG, heat_col_path)\n",
    "\n",
    "ct_dept = profile_department_heatmap(df_prof, cust_prof)\n",
    "plt.figure(figsize=(max(CONFIG['plot']['figsize'][0], 12), max(CONFIG['plot']['figsize'][1], 8)))\n",
    "ax = sns.heatmap(ct_dept, cmap='magma')\n",
    "ax.set_title('Profile vs Department (Row-normalized)')\n",
    "plt.tight_layout()\n",
    "plt.savefig(heat_dept_path)\n",
    "plt.close()\n",
    "\n",
    "# Top departments by profile (table)\n",
    "top_depts = top_departments_by_profile(df_prof, cust_prof, top_n=10)\n",
    "# Save top departments by profile\n",
    "top_depts_base = Path(CONFIG['paths']['data_out']) / 'top_departments_by_profile'\n",
    "export_dataframe(top_depts, top_depts_base)\n",
    "TOP_DEPTS_PARQUET = str(top_depts_base.with_suffix('.parquet'))\n",
    "TOP_DEPTS_CSV = str(top_depts_base.with_suffix('.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56f1fdf",
   "metadata": {},
   "source": [
    "# Export Final Datasets\n",
    "_Save anonymized active sample and the final dataset with derived columns (`region`, `low_activity_flag`, `profile`, `total_spend_per_customer`)._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab57b99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute total_spend_per_customer for row-level merge\n",
    "total_spend = df_prof.groupby('customer_id')['price'].sum().rename('total_spend_per_customer')\n",
    "df_final = df_prof.merge(total_spend, on='customer_id', how='left')\n",
    "\n",
    "final_base = Path(CONFIG['paths']['data_out']) / CONFIG['files']['final_dataset_base']\n",
    "export_dataframe(df_final[[\n",
    "    'customer_id','state','region','orders_count','order_id','order_number','order_hour_of_day','order_dow',\n",
    "    'age','income','dependents','department_id','price','spending_flag','low_activity_flag','profile',\n",
    "    'total_spend_per_customer'\n",
    "]], final_base)\n",
    "FINAL_PARQUET = str(final_base.with_suffix('.parquet'))\n",
    "FINAL_CSV = str(final_base.with_suffix('.csv'))\n",
    "# Append final population flow step\n",
    "pop_flow.append(('Final (export-ready)', len(df_final)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0d958",
   "metadata": {},
   "source": [
    "# Excel Report (7 Tabs)\n",
    "_Create `reports/Instacart_Final_Report.xlsx` with: 1) Details, 2) Population Flow, 3) Consistency Checks, 4) Wrangling & Security, 5) Derived Columns, 6) Visualizations, 7) Results & Recommendations._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa96f2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build tables for report\n",
    "population_flow_df = pd.DataFrame(pop_flow, columns=['Step','Row Count'])\n",
    "\n",
    "# Consistency checks\n",
    "checks = []\n",
    "checks.append(['Duplicate rows (full)', int(df_final.duplicated().sum())])\n",
    "checks.append(['Age in [0, 120]', int(((df_final['age'] < 0) | (df_final['age'] > 120) | df_final['age'].isna()).sum())])\n",
    "checks.append(['Income >= 0', int(((df_final['income'] < 0) | df_final['income'].isna()).sum())])\n",
    "checks.append(['No NaN in key cols', int(df_final[['customer_id','order_id','state','region']].isna().any(axis=1).sum())])\n",
    "ct_sf_region = pd.crosstab(df_final['spending_flag'], df_final['region'])\n",
    "consistency_checks_df = pd.DataFrame(checks, columns=['Check','Issues'])\n",
    "\n",
    "# Wrangling & Security\n",
    "wr_sec_rows = []\n",
    "wr_sec_rows.append(['Column Normalization', json.dumps(norm_info.get('renamed', {}))])\n",
    "wr_sec_rows.append(['Added dependents (missing → 0)', str(norm_info.get('added_dependents', False))])\n",
    "wr_sec_rows.append(['PII Columns Hashed', ', '.join(RUN_META.get('pii_cols', [])) if RUN_META.get('pii_cols') else 'None'])\n",
    "wr_sec_rows.append(['State Mapping Exclusions', json.dumps(RUN_META.get('excluded_states', {}))])\n",
    "wrangling_security_df = pd.DataFrame(wr_sec_rows, columns=['Transformation / Security', 'Details'])\n",
    "\n",
    "# Derived columns\n",
    "derived_rows = [\n",
    "    ['region', 'state', 'US Census region mapping (CONFIG.region_mapping)'],\n",
    "    ['low_activity_flag', 'orders_count', '1 if orders_count < 5 else 0'],\n",
    "    ['profile', 'age, income, dependents, department_id, order_dow, order_hour_of_day', 'Prioritized rule-engine (CONFIG.thresholds and dept_groups)'],\n",
    "    ['total_spend_per_customer', 'price', 'Sum of item prices per customer_id']\n",
    "]\n",
    "derived_columns_df = pd.DataFrame(derived_rows, columns=['Column','Source','Logic'])\n",
    "\n",
    "# Results & Recommendations (brief, auto-generated)\n",
    "anova_sig = tests['anova_p'] < 0.05\n",
    "kruskal_sig = tests['kruskal_p'] < 0.05\n",
    "reco = []\n",
    "reco.append(fill('Regional spend comparison indicates {} differences across regions (ANOVA p={:.4f}, Kruskal p={:.4f}). Tailor promotions by region, focusing on high-spend regions with premium assortments and in low-spend regions with price-sensitive offers.'\n",
    "                 .format('significant' if (anova_sig or kruskal_sig) else 'no statistically significant', tests['anova_p'], tests['kruskal_p']), width=100))\n",
    "top_prof = cust_prof['profile'].value_counts().idxmax()\n",
    "reco.append(fill(f'The most prevalent segment is \"{top_prof}\". Prioritize creative, ad timing, and assortments aligned with this segment\\'s shopping hours and departments.', width=100))\n",
    "reco.append(fill('Use the low-activity flag to exclude low-revenue customers from cost-intensive campaigns while still nurturing them with lightweight, app-based incentives.', width=100))\n",
    "reco.append(fill('Link department merchandising with profiles (see heatmaps) to optimize cross-selling and personalized banners.', width=100))\n",
    "\n",
    "lib_versions = {\n",
    "    'pandas': pd.__version__, 'numpy': np.__version__, 'matplotlib': plt.matplotlib.__version__,\n",
    "    'seaborn': sns.__version__, 'scipy': 'present', 'openpyxl': openpyxl.__version__, 'pyarrow': pa.__version__\n",
    "}\n",
    "RUN_META['generated_at'] = datetime.now().isoformat(timespec='seconds')\n",
    "RUN_META['lib_versions'] = lib_versions\n",
    "RUN_META['pii_note'] = 'Applied SHA-256 hashing with salt to detected PII columns: ' + (', '.join(RUN_META['pii_cols']) if RUN_META.get('pii_cols') else 'None')\n",
    "\n",
    "report_path = Path(CONFIG['paths']['reports']) / CONFIG['files']['excel_report']\n",
    "fig_list = [\n",
    "    ('Boxplot – Spend by Region', Path(box_path)),\n",
    "    ('Violin – Spend by Region', Path(violin_path)),\n",
    "    ('Barplot – Profile Distribution', Path(prof_bar_path)),\n",
    "    ('Heatmap – Profile × Region (row-norm)', Path(heat_row_path)),\n",
    "    ('Heatmap – Profile × Region (col-norm)', Path(heat_col_path)),\n",
    "    ('Heatmap – Profile × Department', Path(heat_dept_path))\n",
    "]\n",
    "write_excel_report(\n",
    "    CONFIG,\n",
    "    meta={\n",
    "        'dataset_name': RUN_META.get('dataset_name',''),\n",
    "        'generated_at': RUN_META.get('generated_at',''),\n",
    "        'lib_versions': json.dumps(RUN_META.get('lib_versions', {})),\n",
    "        'pii_note': RUN_META.get('pii_note','')\n",
    "    },\n",
    "    population_flow=population_flow_df,\n",
    "    consistency_checks=consistency_checks_df,\n",
    "    wrangling_security=wrangling_security_df,\n",
    "    derived_columns=derived_columns_df,\n",
    "    figures=fig_list,\n",
    "    results_reco=reco,\n",
    "    out_path=report_path,\n",
    "    crosstab_sf_region=ct_sf_region,\n",
    "    top_depts=top_depts\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d403e23",
   "metadata": {},
   "source": [
    "# Run Summary\n",
    "_Print paths to saved artifacts and key notes. The full execution log is in `logs/run.log`._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e3ce7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ARTIFACTS = {\n",
    "    'Active sample (parquet)': ACTIVE_SAMPLE_PARQUET,\n",
    "    'Active sample (csv)': ACTIVE_SAMPLE_CSV,\n",
    "    'Final dataset (parquet)': FINAL_PARQUET,\n",
    "    'Final dataset (csv)': FINAL_CSV,\n",
    "    'Report (xlsx)': str(report_path),\n",
    "    'Top depts (parquet)': TOP_DEPTS_PARQUET,\n",
    "    'Top depts (csv)': TOP_DEPTS_CSV,\n",
    "    'Figures dir': str(Path(CONFIG['paths']['figures']).resolve()),\n",
    "    'Log file': str(Path(CONFIG['paths']['logs']) / 'run.log')\n",
    "}\n",
    "summarize_artifacts(ARTIFACTS)\n",
    "LOGGER.info('Pipeline completed successfully.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
